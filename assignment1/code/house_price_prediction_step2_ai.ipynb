{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction - Step 2: Models\n",
    "\n",
    "First, run the data cleaning notebook to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data cleaning notebook\n",
    "%run house_price_prediction_step1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "assignment_dir = os.path.dirname(current_path) if \"code\" in current_path else current_path\n",
    "\n",
    "data_out = os.path.join(assignment_dir, \"data\", \"clean/\")\n",
    "output = os.path.join(assignment_dir, \"output/\")\n",
    "\n",
    "import py_helper_functions as da\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_out + \"airbnb_madrid_25q1_clean.csv\")\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic property characteristics - core drivers of price\n",
    "# NOTE: Neighbourhood excluded for cross-city validity\n",
    "basic_lev = (\n",
    "    \"n_accommodates\",      # Capacity is primary price driver\n",
    "    \"n_beds\",              # Number of beds\n",
    "    \"f_property_type\",     # Apartment vs House\n",
    "    \"f_room_type\",         # Entire place vs Private vs Shared\n",
    "    \"n_days_since\",        # Time on platform (experience proxy)\n",
    "    \"flag_days_since\",     # Flag for imputed values\n",
    ")\n",
    "\n",
    "# Additional property features\n",
    "basic_add = (\n",
    "    \"f_bathroom\",          # Bathroom categories\n",
    "    \"n_bedrooms\",          # Number of bedrooms\n",
    ")\n",
    "\n",
    "# Review information - quality signals\n",
    "reviews = (\n",
    "    \"f_number_of_reviews\",       # Review count categories\n",
    "    \"n_review_scores_rating\",    # Rating score\n",
    "    \"flag_review_scores_rating\", # Flag for imputed ratings\n",
    ")\n",
    "\n",
    "# Polynomial terms\n",
    "poly_lev = (\n",
    "    \"n_accommodates2\",     # Squared term for accommodates\n",
    "    \"ln_accommodates\",     # Log transformation\n",
    ")\n",
    "\n",
    "# Host characteristics\n",
    "host_vars = (\n",
    "    \"d_host_is_superhost\",       # Superhost status\n",
    "    \"d_instant_bookable\",        # Convenience feature\n",
    ")\n",
    "\n",
    "# Amenities\n",
    "amenities = tuple([c for c in data.columns if c.startswith('d_') and 'host' not in c and 'instant' not in c])\n",
    "print(f\"Number of amenity variables: {len(amenities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price by room type and property type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "data.groupby(['f_room_type', 'f_property_type'])['price'].mean().unstack().plot(\n",
    "    kind='bar', ax=axes[0], rot=45\n",
    ")\n",
    "axes[0].set_title('Price by Room Type and Property Type')\n",
    "axes[0].set_ylabel('Mean Price (€)')\n",
    "axes[0].legend(title='Property Type')\n",
    "\n",
    "data.groupby(['f_room_type', pd.cut(data['n_accommodates'], bins=[0,2,4,6,20])])['price'].mean().unstack().plot(\n",
    "    kind='bar', ax=axes[1], rot=45\n",
    ")\n",
    "axes[1].set_title('Price by Room Type and Capacity')\n",
    "axes[1].set_ylabel('Mean Price (€)')\n",
    "axes[1].legend(title='Accommodates')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + 'interactions_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1: Core interactions\n",
    "X1 = (\n",
    "    \"f_room_type*f_property_type\",\n",
    "    \"n_accommodates*f_room_type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modellev1 = \"~ n_accommodates\"\n",
    "modellev2 = \"~ \" + \" + \".join(basic_lev)\n",
    "modellev3 = \"~ \" + \" + \".join(basic_lev + basic_add)\n",
    "modellev4 = \"~ \" + \" + \".join(basic_lev + basic_add + reviews)\n",
    "modellev5 = \"~ \" + \" + \".join(basic_lev + basic_add + reviews + poly_lev)\n",
    "modellev6 = \"~ \" + \" + \".join(basic_lev + basic_add + reviews + poly_lev + host_vars + X1)\n",
    "modellev7 = \"~ \" + \" + \".join(basic_lev + basic_add + reviews + poly_lev + host_vars + X1 + amenities)\n",
    "\n",
    "model_equations = [modellev1, modellev2, modellev3, modellev4, modellev5, modellev6, modellev7]\n",
    "model_names = ['M1: Accommodates only', 'M2: Basic', 'M3: +Property features', \n",
    "               'M4: +Reviews', 'M5: +Polynomials', 'M6: +Host & Interactions', 'M7: +Amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data_work, data_holdout = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {data_work.shape[0]} rows\")\n",
    "print(f\"Holdout set: {data_holdout.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING OLS MODELS WITH 5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv_results = []\n",
    "ols_times = []\n",
    "\n",
    "for i, model in enumerate(model_equations):\n",
    "    start_time = time.time()\n",
    "    cv_result = da.ols_crossvalidator(\"price \" + model, data_work, n_folds)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    ols_times.append(elapsed_time)\n",
    "    \n",
    "    cv_result['Model'] = f'M{i+1}_OLS'\n",
    "    cv_result['Time (s)'] = round(elapsed_time, 2)\n",
    "    cv_results.append(cv_result)\n",
    "    \n",
    "    print(f\"M{i+1}_OLS: Test RMSE = {cv_result['Test RMSE']:.2f}, Time = {elapsed_time:.2f}s\")\n",
    "\n",
    "ols_comparison = (\n",
    "    pd.DataFrame(cv_results)\n",
    "    .round(2)\n",
    "    .assign(\n",
    "        BIC=lambda x: x['BIC'].astype(int),\n",
    "        Coefficients=lambda x: x['Coefficients'].astype(int),\n",
    "    )\n",
    "    [['Model', 'Coefficients', 'R-squared', 'BIC', 'Training RMSE', 'Test RMSE', 'Time (s)']]\n",
    ")\n",
    "\n",
    "print(\"\\n--- OLS Model Comparison ---\")\n",
    "print(ols_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df_melted = ols_comparison.melt(\n",
    "    id_vars='Coefficients', \n",
    "    value_vars=['Test RMSE', 'Training RMSE']\n",
    ")\n",
    "\n",
    "sns.lineplot(data=df_melted, x='Coefficients', y='value', hue='variable', \n",
    "             marker='o', linewidth=2)\n",
    "plt.xlabel('Number of Coefficients')\n",
    "plt.ylabel('RMSE (€)')\n",
    "plt.title('OLS Model Complexity vs. RMSE')\n",
    "plt.legend(title='')\n",
    "plt.savefig(output + 'ols_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the most complex model specification for LASSO\n",
    "vars_for_lasso = modellev7\n",
    "\n",
    "# Create design matrix using patsy\n",
    "y, X = patsy.dmatrices(\"price \" + vars_for_lasso, data_work)\n",
    "X_featnames = X.design_info.column_names\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define range of lambda values\n",
    "lambdas = np.arange(0.05, 1.01, 0.05)\n",
    "\n",
    "# Run LASSO with 5-fold CV\n",
    "start_time = time.time()\n",
    "lasso_fit = LassoCV(alphas=lambdas, cv=5, random_state=42).fit(X, y)\n",
    "lasso_time = time.time() - start_time\n",
    "\n",
    "print(f\"Optimal lambda: {lasso_fit.alpha_}\")\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "y_pred_train = lasso_fit.predict(X)\n",
    "lasso_train_rmse = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "lasso_r2 = r2_score(y, y_pred_train)\n",
    "\n",
    "n = len(y)\n",
    "k = np.sum(lasso_fit.coef_ != 0)\n",
    "mse = mean_squared_error(y, y_pred_train)\n",
    "lasso_bic = n * np.log(mse) + k * np.log(n)\n",
    "\n",
    "print(f\"LASSO Training RMSE: {lasso_train_rmse:.2f}\")\n",
    "print(f\"LASSO R-squared: {lasso_r2:.3f}\")\n",
    "print(f\"LASSO BIC: {lasso_bic:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_by_lambda = (\n",
    "    pd.DataFrame({\n",
    "        'lambda': lasso_fit.alphas_,\n",
    "        'Test RMSE': np.sqrt(lasso_fit.mse_path_).mean(axis=1)\n",
    "    })\n",
    "    .sort_values('lambda')\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.plot(rmse_by_lambda['lambda'], rmse_by_lambda['Test RMSE'], 'b-', linewidth=2)\n",
    "plt.axvline(x=lasso_fit.alpha_, color='r', linestyle='--', label=f'Optimal λ = {lasso_fit.alpha_:.3f}')\n",
    "plt.xlabel('Lambda (Regularization Parameter)')\n",
    "plt.ylabel('Test RMSE (€)')\n",
    "plt.title('LASSO: Cross-Validation RMSE by Lambda')\n",
    "plt.legend()\n",
    "plt.savefig(output + 'lasso_lambda_selection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coefs = pd.DataFrame({\n",
    "    'Feature': X_featnames,\n",
    "    'Coefficient': lasso_fit.coef_\n",
    "})\n",
    "\n",
    "nonzero_coefs = lasso_coefs[np.abs(lasso_coefs['Coefficient']) > 0.001].copy()\n",
    "nonzero_coefs['Abs_Coef'] = np.abs(nonzero_coefs['Coefficient'])\n",
    "nonzero_coefs = nonzero_coefs.sort_values('Abs_Coef', ascending=False)\n",
    "\n",
    "print(f\"\\nLASSO selected {len(nonzero_coefs)} features out of {len(X_featnames)}\")\n",
    "print(\"\\nTop 20 features by absolute coefficient:\")\n",
    "print(nonzero_coefs.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_rmse = rmse_by_lambda.loc[rmse_by_lambda['lambda'] == lasso_fit.alpha_, 'Test RMSE'].values[0]\n",
    "\n",
    "lasso_row = {\n",
    "    'Model': 'LASSO',\n",
    "    'Coefficients': np.sum(lasso_fit.coef_ != 0),\n",
    "    'R-squared': round(lasso_r2, 2),\n",
    "    'BIC': int(lasso_bic),\n",
    "    'Training RMSE': round(lasso_train_rmse, 2),\n",
    "    'Test RMSE': round(lasso_rmse, 2),\n",
    "    'Time (s)': round(lasso_time, 2)\n",
    "}\n",
    "comparison = pd.concat([ols_comparison, pd.DataFrame([lasso_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors for tree models\n",
    "predictors = basic_lev + basic_add + reviews + poly_lev + host_vars + amenities\n",
    "\n",
    "# Create design matrices\n",
    "y_train, X_train = patsy.dmatrices(\"price ~ \" + \" + \".join(predictors), data_work)\n",
    "y_train = np.ravel(y_train)\n",
    "X_featnames = X_train.design_info.column_names\n",
    "\n",
    "print(f\"Training data: {X_train.shape[0]} obs, {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_tune_grid = {\n",
    "    \"max_features\": [5, 8, 10, 12],\n",
    "    \"min_samples_leaf\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=rf_tune_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "rf_cv.fit(X_train, y_train)\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "print(f\"Best params: {rf_cv.best_params_}\")\n",
    "print(f\"Best CV RMSE: {-rf_cv.best_score_:.2f}\")\n",
    "print(f\"Time: {rf_time:.2f}s\")\n",
    "\n",
    "rf_results = pd.DataFrame(rf_cv.cv_results_)[['param_max_features', 'param_min_samples_leaf', 'mean_test_score']]\n",
    "rf_results['RMSE'] = -rf_results['mean_test_score']\n",
    "print(\"\\nRF Grid Search Results:\")\n",
    "print(rf_results.pivot(index='param_max_features', columns='param_min_samples_leaf', values='RMSE').round(2))\n",
    "\n",
    "rf_train_pred = rf_cv.predict(X_train)\n",
    "rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_train_pred))\n",
    "rf_test_rmse = -rf_cv.best_score_\n",
    "rf_r2 = r2_score(y_train, rf_train_pred)\n",
    "n = len(y_train)\n",
    "k = X_train.shape[1]\n",
    "rf_bic = n * np.log(mean_squared_error(y_train, rf_train_pred)) + k * np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "gbm_tune_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"min_samples_leaf\": [10, 20]\n",
    "}\n",
    "\n",
    "gbm_cv = GridSearchCV(\n",
    "    estimator=gbm,\n",
    "    param_grid=gbm_tune_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "gbm_cv.fit(X_train, y_train)\n",
    "gbm_time = time.time() - start_time\n",
    "\n",
    "print(f\"Best params: {gbm_cv.best_params_}\")\n",
    "print(f\"Best CV RMSE: {-gbm_cv.best_score_:.2f}\")\n",
    "print(f\"Time: {gbm_time:.2f}s\")\n",
    "\n",
    "gbm_train_pred = gbm_cv.predict(X_train)\n",
    "gbm_train_rmse = np.sqrt(mean_squared_error(y_train, gbm_train_pred))\n",
    "gbm_test_rmse = -gbm_cv.best_score_\n",
    "gbm_r2 = r2_score(y_train, gbm_train_pred)\n",
    "gbm_bic = n * np.log(mean_squared_error(y_train, gbm_train_pred)) + k * np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbm = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "hgbm_tune_grid = {\n",
    "    \"max_iter\": [100, 200],\n",
    "    \"max_depth\": [5, 10],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"min_samples_leaf\": [10, 20]\n",
    "}\n",
    "\n",
    "hgbm_cv = GridSearchCV(\n",
    "    estimator=hgbm,\n",
    "    param_grid=hgbm_tune_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "hgbm_cv.fit(X_train, y_train)\n",
    "hgbm_time = time.time() - start_time\n",
    "\n",
    "print(f\"Best params: {hgbm_cv.best_params_}\")\n",
    "print(f\"Best CV RMSE: {-hgbm_cv.best_score_:.2f}\")\n",
    "print(f\"Time: {hgbm_time:.2f}s\")\n",
    "\n",
    "hgbm_train_pred = hgbm_cv.predict(X_train)\n",
    "hgbm_train_rmse = np.sqrt(mean_squared_error(y_train, hgbm_train_pred))\n",
    "hgbm_test_rmse = -hgbm_cv.best_score_\n",
    "hgbm_r2 = r2_score(y_train, hgbm_train_pred)\n",
    "hgbm_bic = n * np.log(mean_squared_error(y_train, hgbm_train_pred)) + k * np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_row = {\n",
    "    'Model': 'Random Forest',\n",
    "    'Coefficients': X_train.shape[1],\n",
    "    'R-squared': round(rf_r2, 2),\n",
    "    'BIC': int(rf_bic),\n",
    "    'Training RMSE': round(rf_train_rmse, 2),\n",
    "    'Test RMSE': round(rf_test_rmse, 2),\n",
    "    'Time (s)': round(rf_time, 2)\n",
    "}\n",
    "\n",
    "gbm_row = {\n",
    "    'Model': 'GBM',\n",
    "    'Coefficients': X_train.shape[1],\n",
    "    'R-squared': round(gbm_r2, 2),\n",
    "    'BIC': int(gbm_bic),\n",
    "    'Training RMSE': round(gbm_train_rmse, 2),\n",
    "    'Test RMSE': round(gbm_test_rmse, 2),\n",
    "    'Time (s)': round(gbm_time, 2)\n",
    "}\n",
    "\n",
    "hgbm_row = {\n",
    "    'Model': 'HistGBM',\n",
    "    'Coefficients': X_train.shape[1],\n",
    "    'R-squared': round(hgbm_r2, 2),\n",
    "    'BIC': int(hgbm_bic),\n",
    "    'Training RMSE': round(hgbm_train_rmse, 2),\n",
    "    'Test RMSE': round(hgbm_test_rmse, 2),\n",
    "    'Time (s)': round(hgbm_time, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.concat([\n",
    "    comparison,\n",
    "    pd.DataFrame([rf_row, gbm_row, hgbm_row])\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL COMPARISON: ALL 5 MODELS\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "comparison.to_csv(output + 'model_comparison_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = comparison['Test RMSE'].idxmin()\n",
    "best_model = comparison.loc[best_idx]\n",
    "\n",
    "print(f\"\\nBest model: {best_model['Model']}\")\n",
    "print(f\"  - Test RMSE: €{best_model['Test RMSE']:.2f}\")\n",
    "print(f\"  - Training RMSE: €{best_model['Training RMSE']:.2f}\")\n",
    "print(f\"  - Time: {best_model['Time (s)']:.2f}s\")\n",
    "\n",
    "print(\"\\nModel ranking by Test RMSE:\")\n",
    "ranking = comparison.sort_values('Test RMSE')[['Model', 'Test RMSE', 'Time (s)']]\n",
    "print(ranking.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "rf_var_imp = pd.DataFrame({\n",
    "    'variable': X_featnames,\n",
    "    'imp': rf_cv.best_estimator_.feature_importances_\n",
    "}).sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "\n",
    "rf_var_imp['cumulative_imp'] = rf_var_imp['imp'].cumsum()\n",
    "\n",
    "gbm_var_imp = pd.DataFrame({\n",
    "    'variable': X_featnames,\n",
    "    'imp': gbm_cv.best_estimator_.feature_importances_\n",
    "}).sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "\n",
    "gbm_var_imp['cumulative_imp'] = gbm_var_imp['imp'].cumsum()\n",
    "\n",
    "print(\"Random Forest - Top 10 Features:\")\n",
    "display(rf_var_imp.head(10).style.format({'imp': '{:.1%}', 'cumulative_imp': '{:.1%}'}))\n",
    "print(\"\\nGradient Boosting - Top 10 Features:\")\n",
    "display(gbm_var_imp.head(10).style.format({'imp': '{:.1%}', 'cumulative_imp': '{:.1%}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.01\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "rf_plot_data = rf_var_imp[rf_var_imp.imp > cutoff].sort_values(by='imp')\n",
    "axes[0].barh(rf_plot_data['variable'], rf_plot_data['imp'], color='steelblue')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest: Feature Importances (>1%)')\n",
    "axes[0].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "gbm_plot_data = gbm_var_imp[gbm_var_imp.imp > cutoff].sort_values(by='imp')\n",
    "axes[1].barh(gbm_plot_data['variable'], gbm_plot_data['imp'], color='darkgreen')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Gradient Boosting: Feature Importances (>1%)')\n",
    "axes[1].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + 'feature_importance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_comparison = pd.DataFrame({\n",
    "    'Rank': range(1, 11),\n",
    "    'RF Feature': rf_var_imp.head(10)['variable'].values,\n",
    "    'RF Importance': rf_var_imp.head(10)['imp'].values,\n",
    "    'GBM Feature': gbm_var_imp.head(10)['variable'].values,\n",
    "    'GBM Importance': gbm_var_imp.head(10)['imp'].values\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TOP 10 FEATURES COMPARISON: Random Forest vs Gradient Boosting\")\n",
    "print(\"=\"*70)\n",
    "display(top10_comparison.style.format({\n",
    "    'RF Importance': '{:.1%}', \n",
    "    'GBM Importance': '{:.1%}'\n",
    "}))\n",
    "\n",
    "top10_comparison.to_csv(output + 'top10_features_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_top10 = set(rf_var_imp.head(10)['variable'])\n",
    "gbm_top10 = set(gbm_var_imp.head(10)['variable'])\n",
    "common_features = rf_top10.intersection(gbm_top10)\n",
    "rf_only = rf_top10 - gbm_top10\n",
    "gbm_only = gbm_top10 - rf_top10\n",
    "\n",
    "print(f\"Common features in top 10: {len(common_features)}\")\n",
    "print(f\"  {sorted(common_features)}\")\n",
    "print(f\"\\nOnly in RF top 10: {len(rf_only)}\")\n",
    "print(f\"  {sorted(rf_only)}\")\n",
    "print(f\"\\nOnly in GBM top 10: {len(gbm_only)}\")\n",
    "print(f\"  {sorted(gbm_only)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "SAMPLE_SIZE = 1000\n",
    "np.random.seed(42)\n",
    "sample_idx = np.random.choice(X_train.shape[0], size=min(SAMPLE_SIZE, X_train.shape[0]), replace=False)\n",
    "X_sample = X_train[sample_idx]\n",
    "\n",
    "rf_explainer = shap.TreeExplainer(rf_cv.best_estimator_)\n",
    "gbm_explainer = shap.TreeExplainer(gbm_cv.best_estimator_)\n",
    "\n",
    "print(\"Computing SHAP values for Random Forest...\")\n",
    "rf_shap_values = rf_explainer.shap_values(X_sample)\n",
    "print(\"Computing SHAP values for Gradient Boosting...\")\n",
    "gbm_shap_values = gbm_explainer.shap_values(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "shap.summary_plot(rf_shap_values, X_sample, feature_names=X_featnames, \n",
    "                  plot_type='bar', max_display=10, show=False)\n",
    "plt.title('Random Forest: SHAP Feature Importance', fontsize=14)\n",
    "plt.savefig(output + 'shap_rf.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "shap.summary_plot(gbm_shap_values, X_sample, feature_names=X_featnames, \n",
    "                  plot_type='bar', max_display=10, show=False)\n",
    "plt.title('Gradient Boosting: SHAP Feature Importance', fontsize=14)\n",
    "plt.savefig(output + 'shap_gbm.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_imp = rf_var_imp[['variable', 'imp']].merge(\n",
    "    gbm_var_imp[['variable', 'imp']], \n",
    "    on='variable', \n",
    "    suffixes=('_rf', '_gbm')\n",
    ")\n",
    "correlation = merged_imp['imp_rf'].corr(merged_imp['imp_gbm'])\n",
    "\n",
    "rf_cumulative = rf_var_imp.head(10)['imp'].sum()\n",
    "gbm_cumulative = gbm_var_imp.head(10)['imp'].sum()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY: Feature Importance Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n1. CORRELATION between RF and GBM importances: {correlation:.2f}\")\n",
    "print(f\"\\n2. TOP FEATURE:\")\n",
    "print(f\"   - RF top feature: {rf_var_imp.iloc[0]['variable']} ({rf_var_imp.iloc[0]['imp']:.1%})\")\n",
    "print(f\"   - GBM top feature: {gbm_var_imp.iloc[0]['variable']} ({gbm_var_imp.iloc[0]['imp']:.1%})\")\n",
    "print(f\"\\n3. OVERLAP: {len(common_features)}/10 features shared in both top 10 lists\")\n",
    "print(f\"\\n4. CONCENTRATION:\")\n",
    "print(f\"   - RF: Top 10 features explain {rf_cumulative:.1%} of total importance\")\n",
    "print(f\"   - GBM: Top 10 features explain {gbm_cumulative:.1%} of total importance\")\n",
    "if rf_only or gbm_only:\n",
    "    print(f\"\\n5. KEY DIFFERENCES:\")\n",
    "    if rf_only:\n",
    "        print(f\"   - RF uniquely emphasizes: {', '.join(sorted(rf_only))}\")\n",
    "    if gbm_only:\n",
    "        print(f\"   - GBM uniquely emphasizes: {', '.join(sorted(gbm_only))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Validity Analysis\n",
    "\n",
    "Testing model performance on:\n",
    "- **A. Later date:** Madrid Q2 2025 (temporal validity)\n",
    "- **B. Other city:** Valencia Q1 2025 (external validity)\n",
    "\n",
    "Note: Models are trained WITHOUT neighbourhood to enable cross-city comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Three Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all three cleaned datasets\n",
    "data_madrid_q1 = pd.read_csv(data_out + \"airbnb_madrid_25q1_clean.csv\")\n",
    "data_madrid_q2 = pd.read_csv(data_out + \"airbnb_madrid_25q2_clean.csv\")\n",
    "data_valencia = pd.read_csv(data_out + \"airbnb_valencia_25q1_clean.csv\")\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"  Madrid Q1 2025: {data_madrid_q1.shape[0]:,} observations\")\n",
    "print(f\"  Madrid Q2 2025: {data_madrid_q2.shape[0]:,} observations\")\n",
    "print(f\"  Valencia Q1 2025: {data_valencia.shape[0]:,} observations\")\n",
    "\n",
    "# Check price distributions\n",
    "print(\"\\nPrice statistics:\")\n",
    "for name, df in [('Madrid Q1', data_madrid_q1), ('Madrid Q2', data_madrid_q2), ('Valencia', data_valencia)]:\n",
    "    print(f\"  {name}: mean=€{df['price'].mean():.0f}, median=€{df['price'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Models on Madrid Q1 (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full Madrid Q1 dataset for training (no holdout needed for validity)\n",
    "train_data = data_madrid_q1.copy()\n",
    "\n",
    "# Ensure amenities tuple is based on training data columns\n",
    "amenities_valid = tuple([c for c in train_data.columns if c.startswith('d_') and 'host' not in c and 'instant' not in c])\n",
    "\n",
    "# Full predictor set (no neighbourhood)\n",
    "predictors_valid = basic_lev + basic_add + reviews + poly_lev + host_vars + amenities_valid\n",
    "\n",
    "# Create design matrix for training\n",
    "y_train_valid, X_train_valid = patsy.dmatrices(\"price ~ \" + \" + \".join(predictors_valid), train_data)\n",
    "y_train_valid = np.ravel(y_train_valid)\n",
    "X_featnames_valid = X_train_valid.design_info.column_names\n",
    "\n",
    "print(f\"Training features: {X_train_valid.shape[1]}\")\n",
    "print(f\"Training observations: {X_train_valid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 5 models for validity testing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODELS FOR VALIDITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. OLS (M7 - full model)\n",
    "import statsmodels.formula.api as smf\n",
    "ols_formula = \"price ~ \" + \" + \".join(predictors_valid)\n",
    "ols_model = smf.ols(ols_formula, data=train_data).fit()\n",
    "print(f\"OLS trained: {len(ols_model.params)} coefficients\")\n",
    "\n",
    "# 2. LASSO\n",
    "scaler_valid = StandardScaler()\n",
    "X_scaled = scaler_valid.fit_transform(X_train_valid)\n",
    "lasso_model = LassoCV(alphas=np.arange(0.05, 1.01, 0.05), cv=5, random_state=42).fit(X_scaled, y_train_valid)\n",
    "print(f\"LASSO trained: {np.sum(lasso_model.coef_ != 0)} non-zero coefficients\")\n",
    "\n",
    "# 3. Random Forest (use best params from earlier)\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    max_features=rf_cv.best_params_['max_features'],\n",
    "    min_samples_leaf=rf_cv.best_params_['min_samples_leaf'],\n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ").fit(X_train_valid, y_train_valid)\n",
    "print(f\"Random Forest trained\")\n",
    "\n",
    "# 4. GBM (use best params from earlier)\n",
    "gbm_model = GradientBoostingRegressor(\n",
    "    n_estimators=gbm_cv.best_params_['n_estimators'],\n",
    "    max_depth=gbm_cv.best_params_['max_depth'],\n",
    "    learning_rate=gbm_cv.best_params_['learning_rate'],\n",
    "    min_samples_leaf=gbm_cv.best_params_['min_samples_leaf'],\n",
    "    random_state=42\n",
    ").fit(X_train_valid, y_train_valid)\n",
    "print(f\"GBM trained\")\n",
    "\n",
    "# 5. HistGBM (use best params from earlier)\n",
    "hgbm_model = HistGradientBoostingRegressor(\n",
    "    max_iter=hgbm_cv.best_params_['max_iter'],\n",
    "    max_depth=hgbm_cv.best_params_['max_depth'],\n",
    "    learning_rate=hgbm_cv.best_params_['learning_rate'],\n",
    "    min_samples_leaf=hgbm_cv.best_params_['min_samples_leaf'],\n",
    "    random_state=42\n",
    ").fit(X_train_valid, y_train_valid)\n",
    "print(f\"HistGBM trained\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Evaluate on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def evaluate_on_dataset(dataset, dataset_name, design_info):\n",
    "    \"\"\"Evaluate all 5 models on a dataset\"\"\"\n",
    "    results = {'Dataset': dataset_name}\n",
    "    \n",
    "    # Create design matrix using the SAME design info from training\n",
    "    y_test, X_test = patsy.dmatrices(\"price ~ \" + \" + \".join(predictors_valid), dataset)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    # OLS predictions\n",
    "    ols_pred = ols_model.predict(dataset)\n",
    "    results['OLS'] = calculate_rmse(y_test, ols_pred)\n",
    "    \n",
    "    # LASSO predictions (need to scale)\n",
    "    X_test_scaled = scaler_valid.transform(X_test)\n",
    "    lasso_pred = lasso_model.predict(X_test_scaled)\n",
    "    results['LASSO'] = calculate_rmse(y_test, lasso_pred)\n",
    "    \n",
    "    # Tree model predictions\n",
    "    results['RF'] = calculate_rmse(y_test, rf_model.predict(X_test))\n",
    "    results['GBM'] = calculate_rmse(y_test, gbm_model.predict(X_test))\n",
    "    results['HistGBM'] = calculate_rmse(y_test, hgbm_model.predict(X_test))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all three datasets\n",
    "print(\"Evaluating models on all datasets...\\n\")\n",
    "\n",
    "validity_results = []\n",
    "\n",
    "# Madrid Q1 (training data - for reference)\n",
    "validity_results.append(evaluate_on_dataset(data_madrid_q1, 'Madrid Q1 (train)', X_train_valid.design_info))\n",
    "\n",
    "# Madrid Q2 (temporal validity)\n",
    "validity_results.append(evaluate_on_dataset(data_madrid_q2, 'Madrid Q2 (later)', X_train_valid.design_info))\n",
    "\n",
    "# Valencia (external validity)\n",
    "validity_results.append(evaluate_on_dataset(data_valencia, 'Valencia Q1 (other city)', X_train_valid.design_info))\n",
    "\n",
    "# Create results table\n",
    "validity_df = pd.DataFrame(validity_results)\n",
    "validity_df = validity_df.round(2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDITY ANALYSIS: RMSE BY DATASET AND MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(validity_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "validity_df.to_csv(output + 'validity_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize validity results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "validity_melted = validity_df.melt(id_vars='Dataset', var_name='Model', value_name='RMSE')\n",
    "\n",
    "x = np.arange(len(validity_df))\n",
    "width = 0.15\n",
    "models = ['OLS', 'LASSO', 'RF', 'GBM', 'HistGBM']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    ax.bar(x + i*width, validity_df[model], width, label=model, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('RMSE (€)')\n",
    "ax.set_title('Model Performance Across Datasets (Validity Analysis)')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(validity_df['Dataset'], rotation=15, ha='right')\n",
    "ax.legend(title='Model')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + 'validity_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validity Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE increases\n",
    "train_rmse = validity_df[validity_df['Dataset'] == 'Madrid Q1 (train)'].iloc[0]\n",
    "q2_rmse = validity_df[validity_df['Dataset'] == 'Madrid Q2 (later)'].iloc[0]\n",
    "valencia_rmse = validity_df[validity_df['Dataset'] == 'Valencia Q1 (other city)'].iloc[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDITY ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. TEMPORAL VALIDITY (Madrid Q1 → Q2):\")\n",
    "for model in ['OLS', 'LASSO', 'RF', 'GBM', 'HistGBM']:\n",
    "    pct_change = (q2_rmse[model] - train_rmse[model]) / train_rmse[model] * 100\n",
    "    print(f\"   {model}: €{train_rmse[model]:.2f} → €{q2_rmse[model]:.2f} ({pct_change:+.1f}%)\")\n",
    "\n",
    "print(\"\\n2. EXTERNAL VALIDITY (Madrid → Valencia):\")\n",
    "for model in ['OLS', 'LASSO', 'RF', 'GBM', 'HistGBM']:\n",
    "    pct_change = (valencia_rmse[model] - train_rmse[model]) / train_rmse[model] * 100\n",
    "    print(f\"   {model}: €{train_rmse[model]:.2f} → €{valencia_rmse[model]:.2f} ({pct_change:+.1f}%)\")\n",
    "\n",
    "# Find best model for each scenario\n",
    "print(\"\\n3. BEST MODEL BY DATASET:\")\n",
    "for _, row in validity_df.iterrows():\n",
    "    model_rmses = row[['OLS', 'LASSO', 'RF', 'GBM', 'HistGBM']]\n",
    "    best_model = model_rmses.idxmin()\n",
    "    print(f\"   {row['Dataset']}: {best_model} (RMSE = €{model_rmses[best_model]:.2f})\")\n",
    "\n",
    "print(\"\\n4. KEY FINDINGS:\")\n",
    "avg_temporal = np.mean([(q2_rmse[m] - train_rmse[m]) / train_rmse[m] * 100 for m in models])\n",
    "avg_external = np.mean([(valencia_rmse[m] - train_rmse[m]) / train_rmse[m] * 100 for m in models])\n",
    "print(f\"   - Average RMSE increase (temporal): {avg_temporal:+.1f}%\")\n",
    "print(f\"   - Average RMSE increase (external): {avg_external:+.1f}%\")\n",
    "print(f\"   - Models generalize {'well' if avg_external < 20 else 'with some degradation'} to Valencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "**Temporal Validity (Madrid Q1 → Q2):**\n",
    "- Models trained on Q1 data should perform similarly on Q2 data from the same city\n",
    "- Small RMSE increases suggest stable market conditions\n",
    "- Larger increases might indicate seasonal effects or market changes\n",
    "\n",
    "**External Validity (Madrid → Valencia):**\n",
    "- Testing on a different city evaluates whether the model captures generalizable relationships\n",
    "- Price levels and market dynamics may differ between cities\n",
    "- Excluding neighbourhood allows fair comparison (different neighbourhoods in each city)\n",
    "- Some performance degradation is expected due to city-specific factors\n",
    "\n",
    "**Model Robustness:**\n",
    "- If tree-based models (RF, GBM) degrade more than linear models, they may be overfitting\n",
    "- Simpler models often generalize better to new contexts\n",
    "- The best model depends on the use case: prediction accuracy vs. generalizability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
